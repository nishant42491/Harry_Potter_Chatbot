{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EN-zZi6pHIB"
   },
   "source": [
    "### Install the Huggingface transformers module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-04T10:08:10.386856Z",
     "iopub.status.busy": "2021-11-04T10:08:10.386561Z",
     "iopub.status.idle": "2021-11-04T10:08:19.624788Z",
     "shell.execute_reply": "2021-11-04T10:08:19.623629Z",
     "shell.execute_reply.started": "2021-11-04T10:08:10.386829Z"
    },
    "id": "WD6iOcTmoaxE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ffi (c:\\users\\nishant\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ffi (c:\\users\\nishant\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ffi (c:\\users\\nishant\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ffi (c:\\users\\nishant\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ffi (c:\\users\\nishant\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ffi (c:\\users\\nishant\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "! pip -q install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXuRTjrJo5vk"
   },
   "source": [
    "## Import DialoGPT\n",
    "DialoGPT is a chatbot model made by microsoft. This will be the base for our RickBot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-04T10:08:19.628242Z",
     "iopub.status.busy": "2021-11-04T10:08:19.627792Z",
     "iopub.status.idle": "2021-11-04T10:08:49.225018Z",
     "shell.execute_reply": "2021-11-04T10:08:49.224010Z",
     "shell.execute_reply.started": "2021-11-04T10:08:19.628196Z"
    },
    "id": "FSvzC1j7_Tr8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e8276e36664a688ce9d389847d2b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/641 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ccb4ab4224481db7e9933a3bde8d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f63be5715346039aa00012dd800861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42711609ad3746c0a9a847f840a950c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce0b0957cf024ca091b07945ff840f6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/351M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_size = \"small\" \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"microsoft/DialoGPT-{model_size}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(f\"microsoft/DialoGPT-{model_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45l8_zjlpD5B"
   },
   "source": [
    "## Chat with the untrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-04T10:08:49.227750Z",
     "iopub.status.busy": "2021-11-04T10:08:49.227324Z",
     "iopub.status.idle": "2021-11-04T10:08:54.648772Z",
     "shell.execute_reply": "2021-11-04T10:08:54.647662Z",
     "shell.execute_reply.started": "2021-11-04T10:08:49.227708Z"
    },
    "id": "7NaCfs94pLw4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type \"q\" to quit. Automatically quits after 5 messages\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "MESSAGE:  q\n"
     ]
    }
   ],
   "source": [
    "def chat(model, tokenizer, trained=False):\n",
    "    print(\"type \\\"q\\\" to quit. Automatically quits after 5 messages\")\n",
    "\n",
    "    for step in range(5):\n",
    "        message = input(\"MESSAGE: \")\n",
    "\n",
    "        if message in [\"\", \"q\"]:  # if the user doesn't wanna talk\n",
    "            break\n",
    "\n",
    "        # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "        new_user_input_ids = tokenizer.encode(message + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "        # append the new user input tokens to the chat history\n",
    "        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "        # generated a response while limiting the total chat history to 1000 tokens, \n",
    "        if (trained):\n",
    "            chat_history_ids = model.generate(\n",
    "                bot_input_ids, \n",
    "                max_length=1000,\n",
    "                pad_token_id=tokenizer.eos_token_id,  \n",
    "                no_repeat_ngram_size=3,       \n",
    "                do_sample=True, \n",
    "                top_k=100, \n",
    "                top_p=0.7,\n",
    "                temperature = 0.8, \n",
    "            )\n",
    "        else:\n",
    "            chat_history_ids = model.generate(\n",
    "                bot_input_ids, \n",
    "                max_length=1000, \n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                no_repeat_ngram_size=3\n",
    "            )\n",
    "\n",
    "        # pretty print last ouput tokens from bot\n",
    "        print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n",
    "\n",
    "chat(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Kj2BIaUiS71"
   },
   "source": [
    "## Configuring the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-04T10:18:50.430104Z",
     "iopub.status.busy": "2021-11-04T10:18:50.429684Z",
     "iopub.status.idle": "2021-11-04T10:18:50.442078Z",
     "shell.execute_reply": "2021-11-04T10:18:50.440836Z",
     "shell.execute_reply.started": "2021-11-04T10:18:50.430075Z"
    },
    "id": "jv9TXRvV1HIk"
   },
   "outputs": [],
   "source": [
    "import glob, logging, os, pickle, random, re, torch, pandas as pd, numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from pathlib import Path\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Args to allow for easy convertion of python script to notebook\n",
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.output_dir = f'output-{model_size}'\n",
    "        self.model_type = 'gpt2'\n",
    "        self.model_name_or_path = f'microsoft/DialoGPT-{model_size}'\n",
    "        self.config_name = f'microsoft/DialoGPT-{model_size}'\n",
    "        self.tokenizer_name = f'microsoft/DialoGPT-{model_size}'\n",
    "        self.cache_dir = 'cached'\n",
    "        self.block_size = 512\n",
    "        self.per_gpu_train_batch_size = 4\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.learning_rate = 5e-5\n",
    "        self.weight_decay = 0.0\n",
    "        self.adam_epsilon = 1e-8\n",
    "        self.max_grad_norm = 1.0\n",
    "        self.num_train_epochs = 40  # 3\n",
    "        self.max_steps = -1\n",
    "        self.warmup_steps = 0\n",
    "        self.logging_steps = 1000\n",
    "        self.save_total_limit = None\n",
    "        self.seed = 42\n",
    "        self.local_rank = -1\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-04T10:09:29.274698Z",
     "iopub.status.busy": "2021-11-04T10:09:29.274249Z",
     "iopub.status.idle": "2021-11-04T10:09:29.368363Z",
     "shell.execute_reply": "2021-11-04T10:09:29.367193Z",
     "shell.execute_reply.started": "2021-11-04T10:09:29.274667Z"
    },
    "id": "3dS1radujj2J"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>context1</th>\n",
       "      <th>context2</th>\n",
       "      <th>context3</th>\n",
       "      <th>context4</th>\n",
       "      <th>context5</th>\n",
       "      <th>context6</th>\n",
       "      <th>context7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Do you think it wise to trust Hagrid with some...</td>\n",
       "      <td>Hagrid is bringing him.</td>\n",
       "      <td>And the boy?</td>\n",
       "      <td>The good and the bad.</td>\n",
       "      <td>I'm afraid so, professor.</td>\n",
       "      <td>Are the rumors true, Albus?</td>\n",
       "      <td>Good evening, Professor Dumbledore.</td>\n",
       "      <td>I should've known that you would be here, Prof...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ah, Professor, I would trust Hagrid with my life.</td>\n",
       "      <td>Do you think it wise to trust Hagrid with some...</td>\n",
       "      <td>Hagrid is bringing him.</td>\n",
       "      <td>And the boy?</td>\n",
       "      <td>The good and the bad.</td>\n",
       "      <td>I'm afraid so, professor.</td>\n",
       "      <td>Are the rumors true, Albus?</td>\n",
       "      <td>Good evening, Professor Dumbledore.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Professor Dumbledore, sir.</td>\n",
       "      <td>Ah, Professor, I would trust Hagrid with my life.</td>\n",
       "      <td>Do you think it wise to trust Hagrid with some...</td>\n",
       "      <td>Hagrid is bringing him.</td>\n",
       "      <td>And the boy?</td>\n",
       "      <td>The good and the bad.</td>\n",
       "      <td>I'm afraid so, professor.</td>\n",
       "      <td>Are the rumors true, Albus?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Professor McGonagall.</td>\n",
       "      <td>Professor Dumbledore, sir.</td>\n",
       "      <td>Ah, Professor, I would trust Hagrid with my life.</td>\n",
       "      <td>Do you think it wise to trust Hagrid with some...</td>\n",
       "      <td>Hagrid is bringing him.</td>\n",
       "      <td>And the boy?</td>\n",
       "      <td>The good and the bad.</td>\n",
       "      <td>I'm afraid so, professor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No problems, I trust, Hagrid?</td>\n",
       "      <td>Professor McGonagall.</td>\n",
       "      <td>Professor Dumbledore, sir.</td>\n",
       "      <td>Ah, Professor, I would trust Hagrid with my life.</td>\n",
       "      <td>Do you think it wise to trust Hagrid with some...</td>\n",
       "      <td>Hagrid is bringing him.</td>\n",
       "      <td>And the boy?</td>\n",
       "      <td>The good and the bad.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            response  \\\n",
       "0  Do you think it wise to trust Hagrid with some...   \n",
       "1  Ah, Professor, I would trust Hagrid with my life.   \n",
       "2                         Professor Dumbledore, sir.   \n",
       "3                              Professor McGonagall.   \n",
       "4                      No problems, I trust, Hagrid?   \n",
       "\n",
       "                                            context1  \\\n",
       "0                            Hagrid is bringing him.   \n",
       "1  Do you think it wise to trust Hagrid with some...   \n",
       "2  Ah, Professor, I would trust Hagrid with my life.   \n",
       "3                         Professor Dumbledore, sir.   \n",
       "4                              Professor McGonagall.   \n",
       "\n",
       "                                            context2  \\\n",
       "0                                       And the boy?   \n",
       "1                            Hagrid is bringing him.   \n",
       "2  Do you think it wise to trust Hagrid with some...   \n",
       "3  Ah, Professor, I would trust Hagrid with my life.   \n",
       "4                         Professor Dumbledore, sir.   \n",
       "\n",
       "                                            context3  \\\n",
       "0                              The good and the bad.   \n",
       "1                                       And the boy?   \n",
       "2                            Hagrid is bringing him.   \n",
       "3  Do you think it wise to trust Hagrid with some...   \n",
       "4  Ah, Professor, I would trust Hagrid with my life.   \n",
       "\n",
       "                                            context4  \\\n",
       "0                          I'm afraid so, professor.   \n",
       "1                              The good and the bad.   \n",
       "2                                       And the boy?   \n",
       "3                            Hagrid is bringing him.   \n",
       "4  Do you think it wise to trust Hagrid with some...   \n",
       "\n",
       "                      context5                             context6  \\\n",
       "0  Are the rumors true, Albus?  Good evening, Professor Dumbledore.   \n",
       "1    I'm afraid so, professor.          Are the rumors true, Albus?   \n",
       "2        The good and the bad.            I'm afraid so, professor.   \n",
       "3                 And the boy?                The good and the bad.   \n",
       "4      Hagrid is bringing him.                         And the boy?   \n",
       "\n",
       "                                            context7  \n",
       "0  I should've known that you would be here, Prof...  \n",
       "1                Good evening, Professor Dumbledore.  \n",
       "2                        Are the rumors true, Albus?  \n",
       "3                          I'm afraid so, professor.  \n",
       "4                              The good and the bad.  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../input/harry-potter-final-data/final_data.csv\")\n",
    "\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-04T10:09:31.094975Z",
     "iopub.status.busy": "2021-11-04T10:09:31.094594Z",
     "iopub.status.idle": "2021-11-04T10:09:31.107629Z",
     "shell.execute_reply": "2021-11-04T10:09:31.106371Z",
     "shell.execute_reply.started": "2021-11-04T10:09:31.094944Z"
    },
    "id": "mdjT5EqKkwZb"
   },
   "outputs": [],
   "source": [
    "def construct_conv(row, tokenizer, eos = True):\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))\n",
    "    conv = flatten(conv)\n",
    "    return conv\n",
    "\n",
    "def load_and_cache_examples(args, tokenizer, df_trn):\n",
    "    return ConversationDataset(tokenizer, args, df_trn)\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\n",
    "\n",
    "        block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\n",
    "        directory = args.cache_dir\n",
    "        cached_features_file = os.path.join(directory, args.model_type + \"_cached_lm_\" + str(block_size))\n",
    "\n",
    "        logger.info(\"Creating features from dataset file at %s\", directory)\n",
    "        self.examples = []\n",
    "        for _, row in df.iterrows():\n",
    "            conv = construct_conv(row, tokenizer)\n",
    "            self.examples.append(conv)\n",
    "\n",
    "        logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "        with open(cached_features_file, \"wb\") as handle:\n",
    "            pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.examples[item], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-04T10:09:32.360337Z",
     "iopub.status.busy": "2021-11-04T10:09:32.359901Z",
     "iopub.status.idle": "2021-11-04T10:09:32.383217Z",
     "shell.execute_reply": "2021-11-04T10:09:32.380939Z",
     "shell.execute_reply.started": "2021-11-04T10:09:32.360289Z"
    },
    "id": "6W9ZUG-14pI_"
   },
   "outputs": [],
   "source": [
    "def train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer = SummaryWriter()\n",
    "\n",
    "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "\n",
    "    def collate(examples: List[torch.Tensor]):\n",
    "        if tokenizer._pad_token is None:\n",
    "            return pad_sequence(examples, batch_first=True)\n",
    "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last = True\n",
    "    )\n",
    "\n",
    "    t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    logger.info(\"*** Running trainng, Num examples = %d, Num Epochs = %d ***\", len(train_dataset), args.num_train_epochs)\n",
    "\n",
    "    global_step, epochs_trained = 0, 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(\n",
    "        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n",
    "    )\n",
    "    set_seed(args)  # Added here for reproducibility\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            \n",
    "            inputs, labels = (batch, batch)\n",
    "            if inputs.shape[1] > 1024: continue\n",
    "            inputs = inputs.to(args.device)\n",
    "            labels = labels.to(args.device)\n",
    "            model.train()\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
    "\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    # Log metrics\n",
    "                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "    tb_writer.close()\n",
    "\n",
    "    return global_step, tr_loss / global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-04T10:09:33.073349Z",
     "iopub.status.busy": "2021-11-04T10:09:33.072905Z",
     "iopub.status.idle": "2021-11-04T10:09:33.085222Z",
     "shell.execute_reply": "2021-11-04T10:09:33.083880Z",
     "shell.execute_reply.started": "2021-11-04T10:09:33.073271Z"
    },
    "id": "Jludg4aN4zdc"
   },
   "outputs": [],
   "source": [
    "def main(df_trn):\n",
    "    args = Args()\n",
    "    \n",
    "    # Setup CUDA, GPU & distributed training\n",
    "    device = torch.device(\"cuda\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "    args.device = device\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
    "    )\n",
    "    logger.warning(\"Process rank: %s, device: %s, n_gpu: %s\", args.local_rank, device, args.n_gpu)\n",
    "\n",
    "    set_seed(args) # Set seed\n",
    "\n",
    "    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path, from_tf=False, config=config, cache_dir=args.cache_dir)\n",
    "    model.to(args.device)\n",
    "    \n",
    "    # Training\n",
    "    train_dataset = load_and_cache_examples(args, tokenizer, df_trn)\n",
    "    global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
    "    logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "\n",
    "    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "    logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
    "    model_to_save = (model.module if hasattr(model, \"module\") else model)  # Take care of distributed/parallel training\n",
    "    model_to_save.save_pretrained(args.output_dir)\n",
    "    tokenizer.save_pretrained(args.output_dir)\n",
    "\n",
    "    # Good practice: save your training arguments together with the trained model\n",
    "    torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
    "\n",
    "    # Load a trained model and vocabulary that you have fine-tuned\n",
    "    model = AutoModelForCausalLM.from_pretrained(args.output_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
    "    model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-04T10:19:06.403412Z",
     "iopub.status.busy": "2021-11-04T10:19:06.403008Z"
    },
    "id": "sfTdpQy-5D1n"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2d6ff55a4d94111b124206d2d9415e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "780e5dc5b35b4956b8609433a376b049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ecf8cfc5944c89a3a1bc63226043b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a5fa6671ef3472297c03baee76f48a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba8be1bf0d824be98bc4e9dfab8c9aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06fd665e371e4a769ac4d171516e3af3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f0c7c1947a4ce1928f3e0703eeb81f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0231fbb180c466d9155adb5264f8e2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79b689c8f551478f9a4b12ba1969a829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e01dd385393e407e8d7f9a6bfe9874b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb8143fbce734ebea2ef180faf15dde3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc2c23aa96504e4bbfa362eb73ecfced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18204d73222d4f0480eaaca8dcf3eb08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d424e0b4ebd94e4e949c37238defc6b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d000b986ba4643138675a0507241d035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af0c4ca2315a4193a504b2abc9d053a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb1181ddf0641258c674bc55b10dd93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cac59fcb08a746aca49d6a5dc32d758f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95163a9732f041af8526c9cb30d2ba53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xYlHoEB5Jic"
   },
   "source": [
    "# Chatting with the trained bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-10T13:15:58.079754Z",
     "iopub.status.busy": "2021-08-10T13:15:58.079226Z",
     "iopub.status.idle": "2021-08-10T13:16:28.105018Z",
     "shell.execute_reply": "2021-08-10T13:16:28.103589Z",
     "shell.execute_reply.started": "2021-08-10T13:15:58.079703Z"
    },
    "id": "NkZ0yjsc5LX-"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(f'microsoft/DialoGPT-{model_size}')\n",
    "model = AutoModelForCausalLM.from_pretrained(f'output-{model_size}')\n",
    "chat(model, tokenizer, trained=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
